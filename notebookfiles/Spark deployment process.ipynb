{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark-submit\n",
    "spark-submit  \\\n",
    "#if you have developed code using intellij for scala\n",
    "# you use build.sbt to add dependency jar files but when you move to cluster\n",
    "# some of jars not related to the spark can't be avaialble\n",
    "# so you need to download those jars dynmically while doing spark-submit\n",
    "# lets consider com.typesage%config%1.3.2 jar is not available on cluster\n",
    "# spark-submit -packages groupid:artifact:version\n",
    "-packages com.typesafe:config:1.3.2 \\\n",
    "# By default spark cluster create two executors for any job submission\n",
    "# but this is not supposed to help to run spark job faster because\n",
    "# number of executors needs to be defined based on data volume you are goint to process\n",
    "# if you are processed large volumes of data num-executors should be more\n",
    "# if you are processing small volumes of data num-executors should be less\n",
    "-num-exectors 6\n",
    "# executors are nothing but jvm(CPU,memory)\n",
    "# executor cores will be defined as per EMR/Cluster hardware\n",
    "# out of full RAM memor only some amount of RAM will be allocated to YARN(80%)\n",
    "# suppose if available RAM is 32 GB around , 24GB can be used from each node for YARN/submit job\n",
    "# ITversity lab there are 5 worker nodes(starts with gw01 to gw05)\n",
    "# each node have 32 GB RAM,8 physical Cores CPU\n",
    "-executor-cores 10\n",
    "# classname needs to provide if it has multiple classes\n",
    "-class retail_db.DailyReveneueGenerator\n",
    "# Add jar file which you generated using intellij project\n",
    "-jar /user/rposam2020/project/scala-project.jar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AWS S3 commands\n",
    "# Buckets are like folders/directories\n",
    "aws s3 ls s3://buket_name\n",
    "#to copy files/folders to s3 \n",
    "aws s3 sync localpath/folder_name s3path/folder_name\n",
    "#to copy files/folders from s3\n",
    "aws s3 cp s3path localpath\n",
    "# to see s3 HDFS capacity access using ec2 url\n",
    "#example URL -> ec2-29034.aws.console.com:50070\n",
    "#By default port 22 os disabled on aws cluster\n",
    "# So, we can connect to only master node using public key\n",
    "# To connect to all other nodes(core), we use private key after logging in to master\n",
    "ssh-add -K file.pem master_node\n",
    "# Copy private key from local machine to EMR cluster\n",
    "ssh -A master_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyspark submit can be done using with .py file sameway\n",
    "# if there are any dependency packages then you need to install using pip before submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 and HDFS both are separate\n",
    "# if we go for EMR cluster with Spark and other tools all of them will install in S3(10GB for all software installations)\n",
    "# if you connect to master node then you see\n",
    "# /mnt will have s3, hdfs,mapred,yarn folders\n",
    "# /emr will contain all configuration files of spark and other tools"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
