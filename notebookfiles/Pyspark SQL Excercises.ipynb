{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "import time\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark session with cluster mode(yarn)\n",
    "spark = SparkSession.builder \\\n",
    "     .master(\"yarn\") \\\n",
    "     .appName(\"Pyspark SQL\") \\\n",
    "     .config(\"spark.ui.port\", \"50032\") \\\n",
    "     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.history.kerberos.keytab', 'none'),\n",
       " ('spark.eventLog.enabled', 'true'),\n",
       " ('spark.dynamicAllocation.initialExecutors', '2'),\n",
       " ('spark.history.ui.port', '18081'),\n",
       " ('spark.dynamicAllocation.maxExecutors', '10'),\n",
       " ('spark.driver.extraLibraryPath',\n",
       "  '/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  'http://rm01.itversity.com:19088/proxy/application_1589064448439_1422'),\n",
       " ('spark.executor.extraLibraryPath',\n",
       "  '/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64'),\n",
       " ('spark.executorEnv.PYTHONPATH',\n",
       "  '/usr/hdp/current/spark2-client/python/lib/py4j-0.10.6-src.zip:/usr/hdp/current/spark2-client/python/<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.6-src.zip'),\n",
       " ('spark.history.provider',\n",
       "  'org.apache.spark.deploy.history.FsHistoryProvider'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  'rm01.itversity.com'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.history.fs.cleaner.maxAge', '30d'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.history.fs.cleaner.interval', '1d'),\n",
       " ('spark.ui.filters',\n",
       "  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " ('spark.eventLog.dir', 'hdfs:///spark2-history/'),\n",
       " ('spark.app.name', 'Pyspark SQL'),\n",
       " ('spark.dynamicAllocation.executorIdleTimeout', '120s'),\n",
       " ('spark.ui.proxyBase', '/proxy/application_1589064448439_1422'),\n",
       " ('spark.shuffle.service.enabled', 'true'),\n",
       " ('spark.yarn.queue', 'default'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.history.fs.cleaner.enabled', 'true'),\n",
       " ('spark.driver.host', 'gw03.itversity.com'),\n",
       " ('spark.app.id', 'application_1589064448439_1422'),\n",
       " ('spark.history.fs.logDirectory', 'hdfs:///spark2-history/'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.history.kerberos.principal', 'none'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.appUIAddress', 'http://gw03.itversity.com:50033'),\n",
       " ('spark.dynamicAllocation.minExecutors', '0'),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.driver.port', '36620'),\n",
       " ('spark.dynamicAllocation.enabled', 'true'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.yarn.historyServer.address', 'nn02.itversity.com:18081'),\n",
       " ('spark.sql.warehouse.dir', '/apps/hive/warehouse'),\n",
       " ('spark.ui.port', '50032')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to print all spark configuarations\n",
    "#to get application URL(DAG) use setting PROXY_URI_BASES\n",
    "spark.sparkContext.getConf().getAll() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Spark Context\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create SQL Context\n",
    "sql = spark.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Directly connect to hive Database\n",
    "sql(\"use rposam_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---------+------+----------+------+------+------+\n",
      "| empno| ename|      job|   mgr|  hiredate|   sal|  comm|deptno|\n",
      "+------+------+---------+------+----------+------+------+------+\n",
      "|7369.0| SMITH|    CLERK|7902.0|1980-12-17| 800.0|  null|  20.0|\n",
      "|7499.0| ALLEN| SALESMAN|7698.0|1981-02-20|1600.0| 300.0|  30.0|\n",
      "|7521.0|  WARD| SALESMAN|7698.0|1981-02-22|1250.0| 500.0|  30.0|\n",
      "|7566.0| JONES|  MANAGER|7839.0|1981-04-02|2975.0|  null|  20.0|\n",
      "|7654.0|MARTIN| SALESMAN|7698.0|1981-09-28|1250.0|1400.0|  30.0|\n",
      "|7698.0| BLAKE|  MANAGER|7839.0|1981-05-01|2850.0|  null|  30.0|\n",
      "|7782.0| CLARK|  MANAGER|7839.0|1981-06-09|2450.0|  null|  10.0|\n",
      "|7788.0| SCOTT|  ANALYST|7566.0|1982-12-09|3000.0|  null|  20.0|\n",
      "|7839.0|  KING|PRESIDENT|  null|1981-11-17|5000.0|  null|  10.0|\n",
      "|7844.0|TURNER| SALESMAN|7698.0|1981-09-08|1500.0|   0.0|  30.0|\n",
      "|7876.0| ADAMS|    CLERK|7788.0|1983-01-12|1100.0|  null|  20.0|\n",
      "|7900.0| JAMES|    CLERK|7698.0|1981-12-03| 950.0|  null|  30.0|\n",
      "|7902.0|  FORD|  ANALYST|7566.0|1981-12-03|3000.0|  null|  20.0|\n",
      "|7934.0|MILLER|    CLERK|7782.0|1982-01-23|1300.0|  null|  10.0|\n",
      "+------+------+---------+------+----------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Query some table from hive database\n",
    "sql(\"select * from emp\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp.to_csv(\"output/emp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp.to_excel(\"output/emp.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp.to_json(\"output/emp.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp.to_parquet(\"output/emp.parquet\",engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_csv = spark.read.csv(\"data/emp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Read josn file using pandas(if you use spark then reads from HDFS) from local\n",
    "pandas_df = pd.read_json(\"output/emp.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create Dataframe in Spark using Pandas DataFrame\n",
    "spark_df = spark.createDataFrame(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Registering as temp Table\n",
    "spark_df.createOrReplaceTempView(\"empTab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----------+----+------+------+\n",
      "|empno| ename|      job|   mgr|  hiredate| sal|  comm|deptno|\n",
      "+-----+------+---------+------+----------+----+------+------+\n",
      "| 7369| SMITH|    CLERK|7902.0|1980-12-17| 800|   NaN|    20|\n",
      "| 7499| ALLEN| SALESMAN|7698.0|1981-02-20|1600| 300.0|    30|\n",
      "| 7521|  WARD| SALESMAN|7698.0|1981-02-22|1250| 500.0|    30|\n",
      "| 7566| JONES|  MANAGER|7839.0|1981-04-02|2975|   NaN|    20|\n",
      "| 7654|MARTIN| SALESMAN|7698.0|1981-09-28|1250|1400.0|    30|\n",
      "| 7698| BLAKE|  MANAGER|7839.0|1981-05-01|2850|   NaN|    30|\n",
      "| 7782| CLARK|  MANAGER|7839.0|1981-06-09|2450|   NaN|    10|\n",
      "| 7788| SCOTT|  ANALYST|7566.0|1982-12-09|3000|   NaN|    20|\n",
      "| 7839|  KING|PRESIDENT|   NaN|1981-11-17|5000|   NaN|    10|\n",
      "| 7844|TURNER| SALESMAN|7698.0|1981-09-08|1500|   0.0|    30|\n",
      "| 7876| ADAMS|    CLERK|7788.0|1983-01-12|1100|   NaN|    20|\n",
      "| 7900| JAMES|    CLERK|7698.0|1981-12-03| 950|   NaN|    30|\n",
      "| 7902|  FORD|  ANALYST|7566.0|1981-12-03|3000|   NaN|    20|\n",
      "| 7934|MILLER|    CLERK|7782.0|1982-01-23|1300|   NaN|    10|\n",
      "+-----+------+---------+------+----------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Query using SparkSQL Core\n",
    "sql(\"select * from empTab\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To read json file using Spark from HDFS \n",
    "df = spark.read.json(\"data/emp.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_empdf_collection(df):\n",
    "    l = []\n",
    "    for row in df.collect():\n",
    "        for i in range(0,len(row.empno)):\n",
    "            l.append(\n",
    "                (row.empno[i],\n",
    "                 row.ename[i],\n",
    "                 row.job[i],\n",
    "                 row.mgr[i],\n",
    "                 row.hiredate[i],\n",
    "                 row.sal[i],\n",
    "                 row.comm[i],\n",
    "                 row.deptno[i])\n",
    "            )\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----------+------+------+------+\n",
      "|empno| ename|      job|   mgr|  hiredate|   sal|  comm|deptno|\n",
      "+-----+------+---------+------+----------+------+------+------+\n",
      "| 7369| SMITH|    CLERK|7902.0|1980-12-17| 800.0|  null|    20|\n",
      "| 7499| ALLEN| SALESMAN|7698.0|1981-02-20|1600.0| 300.0|    30|\n",
      "| 7876| ADAMS|    CLERK|7788.0|1983-01-12|1100.0|  null|    20|\n",
      "| 7900| JAMES|    CLERK|7698.0|1981-12-03| 950.0|  null|    30|\n",
      "| 7902|  FORD|  ANALYST|7566.0|1981-12-03|3000.0|  null|    20|\n",
      "| 7934|MILLER|    CLERK|7782.0|1982-01-23|1300.0|  null|    10|\n",
      "| 7521|  WARD| SALESMAN|7698.0|1981-02-22|1250.0| 500.0|    30|\n",
      "| 7566| JONES|  MANAGER|7839.0|1981-04-02|2975.0|  null|    20|\n",
      "| 7654|MARTIN| SALESMAN|7698.0|1981-09-28|1250.0|1400.0|    30|\n",
      "| 7698| BLAKE|  MANAGER|7839.0|1981-05-01|2850.0|  null|    30|\n",
      "| 7782| CLARK|  MANAGER|7839.0|1981-06-09|2450.0|  null|    10|\n",
      "| 7788| SCOTT|  ANALYST|7566.0|1982-12-09|3000.0|  null|    20|\n",
      "| 7839|  KING|PRESIDENT|  null|1981-11-17|5000.0|  null|    10|\n",
      "| 7844|TURNER| SALESMAN|7698.0|1981-09-08|1500.0|   0.0|    30|\n",
      "+-----+------+---------+------+----------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_l = convert_empdf_collection(df)\n",
    "emp_df = spark.createDataFrame(emp_l,(\"empno\",\"ename\",\"job\",\"mgr\",\"hiredate\",\"sal\",\"comm\",\"deptno\"))\n",
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----------+------+----+------+\n",
      "|empno| ename|      job|   mgr|  hiredate|   sal|comm|deptno|\n",
      "+-----+------+---------+------+----------+------+----+------+\n",
      "| 7934|MILLER|    CLERK|7782.0|1982-01-23|1300.0|null|    10|\n",
      "| 7782| CLARK|  MANAGER|7839.0|1981-06-09|2450.0|null|    10|\n",
      "| 7839|  KING|PRESIDENT|  null|1981-11-17|5000.0|null|    10|\n",
      "+-----+------+---------+------+----------+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df[emp_df.deptno == 10].show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
