{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    from pyspark.sql import SparkSession\n",
    "    import pyspark.sql.functions as f\n",
    "    import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    spark =  SparkSession.builder \\\n",
    "         .master('yarn') \\\n",
    "         .appName(\"Pyspark Interview Practises\") \\\n",
    "         .config(\"spark.ui.port\",\"56236\") \\\n",
    "         .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is the difference between map and flatMap and a good use case for each?\n",
    "    rdd = sc.parallelize([\"Roses are red\", \"Violets are blue\"])\n",
    "###### Map splits each value to list/collection two dimensional\n",
    "    rdd.map( lambda x:x.split(\" \")).collect()\n",
    "     [['Roses', 'are', 'red'], ['Violets', 'are', 'blue']]\n",
    "\n",
    "###### Flatmap splits all values to single collection one dimensional\n",
    "    rdd.flatMap(lambda x :x.split(\" \")).collect()\n",
    "     ['Roses', 'are', 'red', 'Violets', 'are', 'blue']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spark - repartition() vs coalesce()\n",
    "1. Repartition can increase/decrease no of partitions and shuffles the data over the cluster\n",
    "2. But coalesce only decrease the no of partitions and don't shuffle the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "1. There is 30 GB file which is processed in 6 mins and generated 1.5 GB file\n",
    "1. with cluster setup 2 Name Nodes, 3 Gateway Nodes, 5 Worker/Data nodes, 1 resource manager\n",
    "1. Gateway nodes each one has 16 cores, 64 GB RAM's,\n",
    "1. All others nodes each one has 8 Cores, 32 GB RAM\n",
    "1. This big file has 270 blocks\n",
    "1. Each block it executes 1 task and 270 task will create to process this file\n",
    "1. Produces same 270 as output files because each block will be considered as one partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are workers, executors, cores in Spark Standalone cluster?\n",
    "\n",
    "##### DRIVER\n",
    "1. The driver is the process where the main method runs. First it converts the user program into tasks and after that it schedules the tasks on the executors\n",
    "\n",
    "##### EXECUTORS\n",
    "1. Executors are worker nodes' processes in charge of running individual tasks in a given Spark job.\n",
    "2. Once they have run the task they send the results to the driver.\n",
    "\n",
    "##### APPLICATION EXECUTION FLOW\n",
    " when you submit an application to the cluster with spark-submit this is what happens internally:\n",
    "\n",
    "1. A standalone application starts and instantiates a SparkContext instance (a driver).\n",
    "2. The driver program ask for resources to the cluster manager to launch executors.\n",
    "3. The cluster manager launches executors.\n",
    "4. The driver process runs through the user application. Depending on the actions and transformations over RDDs task are sent to executors.\n",
    "5. Executors run the tasks and save the results.\n",
    "6. If any worker crashes, its tasks will be sent to different executors to be processed again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the difference between cache and persist?\n",
    "1. Cache() and persist() both the methods are used to improve performance of spark computation. These methods help to save intermediate results so they can be reused in subsequent stages.\n",
    "\n",
    "2. The only difference between cache() and persist() is ,using Cache technique we can save intermediate results in memory only when needed while in Persist() we can save the intermediate results in 5 storage levels(MEMORY_ONLY, MEMORY_AND_DISK, MEMORY_ONLY_SER, MEMORY_AND_DISK_SER, DISK_ONLY).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### How to read multiple text files into a single RDD?\n",
    "    sc.textFile(\"file1,file2\")\n",
    "    sc.textFile(\"folder1,folder2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to stop INFO messages displaying on spark console?\n",
    "##### log4j.properties\n",
    "1. Edit your conf/log4j.properties file and change the following line:\n",
    "2. change \"log4j.rootCategory=INFO, console\" to \"log4j.rootCategory=ERROR, console\"\n",
    "\n",
    "##### Using Spark-shell\n",
    "     import org.apache.log4j.Logger\n",
    "     import org.apache.log4j.Level\n",
    "\n",
    "     Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "     Logger.getLogger(\"akka\").setLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RDDs support two types of operations: \n",
    "1. transformations, which create a new dataset from an existing one, and \n",
    "2. actions, which return a value to the driver program after running a computation on the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark job internal execution\n",
    "    1. When a SparkContext is created, each worker node starts an executor. Executors are separate processes (JVM), that connects back to the driver program. Each executor has the jar of the driver program. Quitting a driver, shuts down the executors. \n",
    "    \n",
    "    2. Each executor can hold some partitions.\n",
    "    \n",
    "    3. When a job is executed, an execution plan is created according to the lineage graph.\n",
    "    \n",
    "    4. The execution job is split into stages, where stages containing as many neighbouring (in the lineage graph) transformations and action, but no shuffles. Thus stages are separated by shuffles.\n",
    "    \n",
    "    5. A task is a command sent from the driver to an executor by serializing the Function object.\n",
    "    \n",
    "    6. The executor deserializes (with the driver jar) the command (task) and executes it on a partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to overwrite the output directory in spark\n",
    "     set(\"spark.hadoop.validateOutputSpecs\", \"false\") \n",
    "     df.write.mode(SaveMode.Overwrite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do the numbers on the progress bar mean in spark-shell?\n",
    "     (14174 + 5) / 62500] is (numCompletedTasks + numActiveTasks) / totalNumOfTasksInThisStage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to save DataFrame directly to Hive?\n",
    "     emp = spark.read.csv(\"data/emp.csv\") \n",
    "     emp.write.saveAsTable(\"rposam_db.emp_from_df\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write to multiple outputs by key Spark - one Spark job\n",
    "     emp_df.write.partitionBy(\"ename\").csv(\"output/emp_by_ename\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the difference between spark checkpoint and persist to a disk\n",
    "1. Persist(MEMORY_AND_DISK) will store the data frame to disk and memory temporary without breaking the lineage of the program\n",
    "1. checkpoint(), on the other hand, breaks lineage and forces data frame to be stored on disk. Unlike usage of cache()/persist(), frequent check-pointing can slow down your program.\n",
    "    ###### Checkpoints are recommended to use when \n",
    "    1. working in an unstable environment to allow fast recovery from failures \n",
    "    1. storing intermediate states of calculation when new entries of the RDD are dependent on the previous entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate two PySpark dataframes\n",
    "###### df1.union(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### AWS file reading from spark\n",
    "    sc.textFile(\"s3n://bucketname/Filename\") now raises another error:\n",
    "    java.lang.IllegalArgumentException: AWS Access Key ID and Secret Access Key must be specified as the username or password (respectively) of a s3n URL, or by setting the fs.s3n.awsAccessKeyId or fs.s3n.awsSecretAccessKey properties (respectively).\n",
    "    lyrics = sc.textFile(\"s3n://MyAccessKeyID:MySecretKey@zpub01/SafeAndSound_Lyrics.txt\")\n",
    "    OR\n",
    "    sc.hadoopConfiguration.set(\"fs.s3n.awsAccessKeyId\", \"BLABLA\")\n",
    "    sc.hadoopConfiguration.set(\"fs.s3n.awsSecretAccessKey\", \"....\") // can contain \"/\"\n",
    "    val myRDD = sc.textFile(\"s3n://myBucket/MyFilePattern\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    spark_home = os.environ.get('SPARK_HOME', None)\n",
    "    spark_home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to speed up spark df.write jdbc to postgres database?\n",
    "1. Approach 1\n",
    "\n",
    "         sqoop export --connect jdbc:postgresql:hostname:port/postgresDB --table target_table --export-dir s3://mybucket/myinputfiles/ --driver org.postgresql.Driver --username master --password password --input-null-string '\\\\N' --input-null-non-string '\\\\N' --direct -m 16 \n",
    "2. Approach 2\n",
    "   Perform repartition on Datafarme so that there would multiple executor writing to DB in parallel\n",
    "\n",
    "          df\n",
    "          .repartition(10)        // No. of concurrent connection Spark to PostgreSQL\n",
    "          .write.format('jdbc').options(\n",
    "          url=psql_url_spark,\n",
    "          driver=spark_env['PSQL_DRIVER'],\n",
    "          dbtable=\"{schema}.{table}\".format(schema=schema, table=table),\n",
    "          user=spark_env['PSQL_USER'],\n",
    "          password=spark_env['PSQL_PASS'],\n",
    "          batchsize=2000000,\n",
    "          queryTimeout=690\n",
    "          ).mode(mode).save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to check if a Hive table exists using PySpark\n",
    "    \"your_table\" in [t.name for t in spark.catalog.listTables(\"default\")] == True \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spark. Problem when writing a large file on aws s3a storage\n",
    "    I am having the same issue: Spark 2.2.0 using hadoop 2.7.2. I run pyspark --driver-memory 16g --executor-memory 16g --packages com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.2 to start pyspark and when I try uploading a small file to S3, it works fine, but when I try a large file (around 10 GB), it throws a confusing 403 Error.\n",
    "    \n",
    "    Updated hadoop from 2.7.3 to 2.8.5 and now everything works without errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How can I read from S3 in pyspark running in local mode?\n",
    "##### I am using PyCharm 2018.1 using Python 3.4 with Spark 2.3 installed via pip in a virtualenv. There is no hadoop installation on the local host, so there is no Spark installation (thus no SPARK_HOME, HADOOP_HOME, etc.)\n",
    "\n",
    "    import os\n",
    "    os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages \"org.apache.hadoop:hadoop-aws:3.1.0\" pyspark-shell'\n",
    "    from pyspark import SparkConf\n",
    "    from pyspark import SparkContext\n",
    "    conf = SparkConf()\\\n",
    "        .setMaster(\"local\")\\\n",
    "        .setAppName(\"pyspark-unittests\")\\\n",
    "        .set(\"spark.sql.parquet.compression.codec\", \"snappy\")\n",
    "    sc = SparkContext(conf = conf)\n",
    "    inputFile = sparkContext.textFile(\"s3://somebucket/file.csv\")\n",
    "    \n",
    "##### Solution:\n",
    "    os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages \"org.apache.hadoop:hadoop-aws:2.7.3\" pyspark-shell'\n",
    "\n",
    "\n",
    "    # Only needed if you use s3://\n",
    "    sc._jsc.hadoopConfiguration().set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    sc._jsc.hadoopConfiguration().set('fs.s3a.access.key', 'awsKey')\n",
    "    sc._jsc.hadoopConfiguration().set('fs.s3a.secret.key', 'awsSecret')\n",
    "    s3File = sc.textFile(\"s3a://myrepo/test.csv\")\n",
    "\n",
    "    print(s3File.count())\n",
    "    print(s3File.id())\n",
    "   \n",
    "##### Solution 2:\n",
    "    Add following lines to your spark config file, for my local pyspark, it is /usr/local/spark/conf/spark-default.conf\n",
    "\n",
    "    spark.hadoop.fs.s3a.access.key=<your access key>\n",
    "    spark.hadoop.fs.s3a.secret.key=<your secret key>\n",
    "    python file content:\n",
    "\n",
    "    from __future__ import print_function\n",
    "    import os\n",
    "\n",
    "    from pyspark import SparkConf\n",
    "    from pyspark import SparkContext\n",
    "\n",
    "    os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3\"\n",
    "    os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How can I get the file-name list of a directory from hdfs in pyspark? [closed]\n",
    "    org.apache.hadoop.fs.FileSystem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pyspark UDF registration \n",
    "    def getTuple(data,separator):\n",
    "        t = tuple()\n",
    "        l = list()\n",
    "        for row in data.split(separator):\n",
    "            l.append(row)\n",
    "        t = tuple(l)\n",
    "        return t\n",
    "        \n",
    "    spark.udf.register('convertStringToTuple',getTuple)\n",
    "    \n",
    "    line = getTuple(\"10 HR Hyderabad\",\" \")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
