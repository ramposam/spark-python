{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "#Spark session with cluster mode(yarn)\n",
    "spark = SparkSession.builder \\\n",
    "     .master(\"yarn\") \\\n",
    "     .appName(\"Pyspark JDBC\") \\\n",
    "     .config(\"spark.ui.port\", \"50032\") \\\n",
    "     .config(\"spark.jars.packages\",\"mysql:mysql-connector-java:5.1.44\") \\\n",
    "     .config(\"spark.jars\",\"jars/mysql-connector-java-5.1.44.jar\") \\\n",
    "     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_mysql = spark. \\\n",
    "    read. \\\n",
    "    format(\"jdbc\"). \\\n",
    "    option(\"url\",\"jdbc:mysql://ms.itversity.com/retail_export\"). \\\n",
    "    option(\"driver\",\"com.mysql.jdbc.Driver\"). \\\n",
    "    option(\"user\",\"retail_user\"). \\\n",
    "    option(\"password\",\"itversity\"). \\\n",
    "    option(\"dbtable\",\"emp\"). \\\n",
    "    load()\n",
    "# spark make use of scala library/java library because it runs on jvm(spark written on scala)\n",
    "# if you use python mysql package also doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- empno: decimal(4,0) (nullable = true)\n",
      " |-- ename: string (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- mgr: decimal(4,0) (nullable = true)\n",
      " |-- hiredate: date (nullable = true)\n",
      " |-- sal: decimal(7,2) (nullable = true)\n",
      " |-- comm: decimal(7,2) (nullable = true)\n",
      " |-- deptno: decimal(2,0) (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_mysql.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create table on jdbc  using indirect api\n",
    "emp_mysql. \\\n",
    "    write.format(\"jdbc\"). \\\n",
    "    option(\"url\",\"jdbc:mysql://ms.itversity.com/retail_export\"). \\\n",
    "    option(\"driver\",\"com.mysql.jdbc.Driver\"). \\\n",
    "    option(\"user\",\"retail_user\"). \\\n",
    "    option(\"password\",\"itversity\"). \\\n",
    "    option(\"dbtable\",\"pyspark_created_table_emp\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# direct api\n",
    "jdbc_props = { 'user' : 'retail_user', 'password' : 'itversity','Driver':\"com.mysql.jdbc.Driver\" }\n",
    "emp_mysql.write.jdbc(url =\"jdbc:mysql://ms.itversity.com/retail_export\",table=\"pyspark_created_temp_table\",\n",
    "                    mode=None,properties=jdbc_props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert into table on jdbc \n",
    "emp_mysql. \\\n",
    "    write.format(\"jdbc\"). \\\n",
    "    option(\"url\",\"jdbc:mysql://ms.itversity.com/retail_export\"). \\\n",
    "    option(\"driver\",\"com.mysql.jdbc.Driver\"). \\\n",
    "    option(\"user\",\"retail_user\"). \\\n",
    "    option(\"password\",\"itversity\"). \\\n",
    "    mode(\"append\"). \\\n",
    "    option(\"dbtable\",\"pyspark_created_temp_table\"). \\\n",
    "    save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Json file\n",
    "emp_json = spark.read.format(\"json\").load(\"data/employees.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------+------+---+-----------+--------------------+\n",
      "|           addresses|               email|first_name|gender| id|  last_name|       phone_numbers|\n",
      "+--------------------+--------------------+----------+------+---+-----------+--------------------+\n",
      "|[[Ridgely, 21684,...|     wsmyth0@loc.gov|    Weidar|  Male|  1|      Smyth|      [550-543-4729]|\n",
      "|[[Waco, 76796, Te...|   eogborn1@wisc.edu|     Emmit|  Male|  2|     Ogborn|[374-344-7772, 42...|\n",
      "|[[Fort Worth, 761...|mdadswell2@edublo...|     Micah|  Male|  3|   Dadswell|[846-266-0132, 23...|\n",
      "|[[Nashville, 3720...|  bcholwell3@who.int|      Berk|  Male|  4|   Cholwell|[524-533-7218, 92...|\n",
      "|[[Austin, 78744, ...|hbasterfield4@int...|     Haley|  Male|  5|Basterfield|[558-470-2433, 90...|\n",
      "|[[New Haven, 0652...|   sbezley5@xing.com|   Siouxie|Female|  6|     Bezley|[135-434-5299, 51...|\n",
      "|[[Fort Myers, 339...| ugrewes6@drupal.org|      Ursa|Female|  7|     Grewes|[366-797-5110, 22...|\n",
      "|[[Idaho Falls, 83...|  aellaman7@usda.gov|     Aviva|Female|  8|    Ellaman|[367-676-4773, 23...|\n",
      "|[[Mobile, 36641, ...|dcoots8@omniture.com|      Dina|Female|  9|      Coots|      [420-812-5769]|\n",
      "|[[Atlanta, 31136,...|tharnes9@cargocol...|   Terrill|  Male| 10|     Harnes|[545-327-8386, 33...|\n",
      "+--------------------+--------------------+----------+------+---+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_json.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- addresses: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- city: string (nullable = true)\n",
      " |    |    |-- postal_code: string (nullable = true)\n",
      " |    |    |-- state: string (nullable = true)\n",
      " |    |    |-- street_name: string (nullable = true)\n",
      " |    |    |-- street_number: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- phone_numbers: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_json.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+------+--------------------+--------------------+--------------------+--------------------+\n",
      "|first_name|  last_name|gender|                city|         postal_code|               state|       phone_numbers|\n",
      "+----------+-----------+------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    Weidar|      Smyth|  Male|           [Ridgely]|             [21684]|          [Maryland]|      [550-543-4729]|\n",
      "|     Emmit|     Ogborn|  Male|   [Waco, Milwaukee]|      [76796, 53205]|  [Texas, Wisconsin]|[374-344-7772, 42...|\n",
      "|     Micah|   Dadswell|  Male|[Fort Worth, New ...|[76162, 10009, 72...|[Texas, New York,...|[846-266-0132, 23...|\n",
      "|      Berk|   Cholwell|  Male|         [Nashville]|             [37205]|         [Tennessee]|[524-533-7218, 92...|\n",
      "|     Haley|Basterfield|  Male|            [Austin]|             [78744]|             [Texas]|[558-470-2433, 90...|\n",
      "|   Siouxie|     Bezley|Female|[New Haven, Brea,...|[06520, 92822, 40...|[Connecticut, Cal...|[135-434-5299, 51...|\n",
      "|      Ursa|     Grewes|Female|[Fort Myers, New ...|[33906, 70142, 92...|[Florida, Louisia...|[366-797-5110, 22...|\n",
      "|     Aviva|    Ellaman|Female|[Idaho Falls, Bir...|[83405, 35225, 32...|[Idaho, Alabama, ...|[367-676-4773, 23...|\n",
      "|      Dina|      Coots|Female|    [Mobile, Durham]|      [36641, 27705]|[Alabama, North C...|      [420-812-5769]|\n",
      "|   Terrill|     Harnes|  Male|[Atlanta, Los Ang...|[31136, 90030, 80...|[Georgia, Califor...|[545-327-8386, 33...|\n",
      "+----------+-----------+------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query json columns\n",
    "from pyspark.sql import functions as f\n",
    "emp_json.select('first_name',\n",
    "                'last_name',\n",
    "                'gender',\n",
    "                'addresses.city',\n",
    "                'addresses.postal_code',\n",
    "                'addresses.state',\n",
    "                'phone_numbers').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save os ORC format in HDFS using PartitionBy\n",
    "emp_mysql.write.format(\"orc\").partitionBy(\"deptno\").save(\"output/mysql_emp_to_orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save os ORC format in HDFS using PartitionBy multiple columns\n",
    "\n",
    "emp_mysql.write.format(\"orc\").partitionBy(\"deptno\",\"job\").save(\"output/mysql_emp_to_orc_partition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.history.kerberos.keytab', 'none'),\n",
       " ('spark.eventLog.enabled', 'true'),\n",
       " ('spark.dynamicAllocation.initialExecutors', '2'),\n",
       " ('spark.history.ui.port', '18081'),\n",
       " ('spark.jars.packages', 'mysql:mysql-connector-java:5.1.44'),\n",
       " ('spark.driver.extraLibraryPath',\n",
       "  '/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64'),\n",
       " ('spark.dynamicAllocation.maxExecutors', '10'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  'http://rm01.itversity.com:19088/proxy/application_1589064448439_13879'),\n",
       " ('spark.yarn.dist.pyFiles',\n",
       "  'file:///home/rposam2020/.ivy2/jars/mysql_mysql-connector-java-5.1.44.jar'),\n",
       " ('spark.executor.extraLibraryPath',\n",
       "  '/usr/hdp/current/hadoop-client/lib/native:/usr/hdp/current/hadoop-client/lib/native/Linux-amd64-64'),\n",
       " ('spark.driver.appUIAddress', 'http://gw03.itversity.com:50032'),\n",
       " ('spark.history.provider',\n",
       "  'org.apache.spark.deploy.history.FsHistoryProvider'),\n",
       " ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  'rm01.itversity.com'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.history.fs.cleaner.maxAge', '30d'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.history.fs.cleaner.interval', '1d'),\n",
       " ('spark.ui.filters',\n",
       "  'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " ('spark.eventLog.dir', 'hdfs:///spark2-history/'),\n",
       " ('spark.dynamicAllocation.executorIdleTimeout', '120s'),\n",
       " ('spark.shuffle.service.enabled', 'true'),\n",
       " ('spark.driver.port', '43466'),\n",
       " ('spark.app.id', 'application_1589064448439_13879'),\n",
       " ('spark.jars', 'jars/mysql-connector-java-5.1.44.jar'),\n",
       " ('spark.yarn.queue', 'default'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///home/rposam2020/.ivy2/jars/mysql_mysql-connector-java-5.1.44.jar'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.ui.proxyBase', '/proxy/application_1589064448439_13878'),\n",
       " ('spark.app.name', 'Pyspark JDBC'),\n",
       " ('spark.history.fs.cleaner.enabled', 'true'),\n",
       " ('spark.driver.host', 'gw03.itversity.com'),\n",
       " ('spark.executorEnv.PYTHONPATH',\n",
       "  '/usr/hdp/current/spark2-client/python/lib/py4j-0.10.6-src.zip:/usr/hdp/current/spark2-client/python/<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.6-src.zip<CPS>{{PWD}}/mysql_mysql-connector-java-5.1.44.jar'),\n",
       " ('spark.history.fs.logDirectory', 'hdfs:///spark2-history/'),\n",
       " ('spark.master', 'yarn'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.history.kerberos.principal', 'none'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.dynamicAllocation.minExecutors', '0'),\n",
       " ('spark.yarn.secondary.jars', 'mysql_mysql-connector-java-5.1.44.jar'),\n",
       " ('spark.yarn.dist.jars',\n",
       "  'file:///home/rposam2020/.ivy2/jars/mysql_mysql-connector-java-5.1.44.jar'),\n",
       " ('spark.yarn.isPython', 'true'),\n",
       " ('spark.submit.pyFiles',\n",
       "  '/home/rposam2020/.ivy2/jars/mysql_mysql-connector-java-5.1.44.jar'),\n",
       " ('spark.dynamicAllocation.enabled', 'true'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.yarn.historyServer.address', 'nn02.itversity.com:18081'),\n",
       " ('spark.sql.warehouse.dir', '/apps/hive/warehouse'),\n",
       " ('spark.ui.port', '50032')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to print all spark configuarations\n",
    "#to get application URL(DAG) use setting PROXY_URI_BASES\n",
    "spark.sparkContext.getConf().getAll() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Spark Context\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create SQL Context\n",
    "sql = spark.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Directly connect to hive Database\n",
    "sql(\"use rposam_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---------+------+----------+------+------+------+\n",
      "| empno| ename|      job|   mgr|  hiredate|   sal|  comm|deptno|\n",
      "+------+------+---------+------+----------+------+------+------+\n",
      "|7369.0| SMITH|    CLERK|7902.0|1980-12-17| 800.0|  null|  20.0|\n",
      "|7499.0| ALLEN| SALESMAN|7698.0|1981-02-20|1600.0| 300.0|  30.0|\n",
      "|7521.0|  WARD| SALESMAN|7698.0|1981-02-22|1250.0| 500.0|  30.0|\n",
      "|7566.0| JONES|  MANAGER|7839.0|1981-04-02|2975.0|  null|  20.0|\n",
      "|7654.0|MARTIN| SALESMAN|7698.0|1981-09-28|1250.0|1400.0|  30.0|\n",
      "|7698.0| BLAKE|  MANAGER|7839.0|1981-05-01|2850.0|  null|  30.0|\n",
      "|7782.0| CLARK|  MANAGER|7839.0|1981-06-09|2450.0|  null|  10.0|\n",
      "|7788.0| SCOTT|  ANALYST|7566.0|1982-12-09|3000.0|  null|  20.0|\n",
      "|7839.0|  KING|PRESIDENT|  null|1981-11-17|5000.0|  null|  10.0|\n",
      "|7844.0|TURNER| SALESMAN|7698.0|1981-09-08|1500.0|   0.0|  30.0|\n",
      "|7876.0| ADAMS|    CLERK|7788.0|1983-01-12|1100.0|  null|  20.0|\n",
      "|7900.0| JAMES|    CLERK|7698.0|1981-12-03| 950.0|  null|  30.0|\n",
      "|7902.0|  FORD|  ANALYST|7566.0|1981-12-03|3000.0|  null|  20.0|\n",
      "|7934.0|MILLER|    CLERK|7782.0|1982-01-23|1300.0|  null|  10.0|\n",
      "+------+------+---------+------+----------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Query some table from hive database\n",
    "sql(\"select * from emp\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp.to_csv(\"output/emp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp.to_excel(\"output/emp.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp.to_json(\"output/emp.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp.to_parquet(\"output/emp.parquet\",engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_csv = spark.read.csv(\"data/emp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Read josn file using pandas(if you use spark then reads from HDFS) from local\n",
    "pandas_df = pd.read_json(\"output/emp.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Create Dataframe in Spark using Pandas DataFrame\n",
    "spark_df = spark.createDataFrame(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Registering as temp Table\n",
    "spark_df.createOrReplaceTempView(\"empTab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----------+----+------+------+\n",
      "|empno| ename|      job|   mgr|  hiredate| sal|  comm|deptno|\n",
      "+-----+------+---------+------+----------+----+------+------+\n",
      "| 7369| SMITH|    CLERK|7902.0|1980-12-17| 800|   NaN|    20|\n",
      "| 7499| ALLEN| SALESMAN|7698.0|1981-02-20|1600| 300.0|    30|\n",
      "| 7521|  WARD| SALESMAN|7698.0|1981-02-22|1250| 500.0|    30|\n",
      "| 7566| JONES|  MANAGER|7839.0|1981-04-02|2975|   NaN|    20|\n",
      "| 7654|MARTIN| SALESMAN|7698.0|1981-09-28|1250|1400.0|    30|\n",
      "| 7698| BLAKE|  MANAGER|7839.0|1981-05-01|2850|   NaN|    30|\n",
      "| 7782| CLARK|  MANAGER|7839.0|1981-06-09|2450|   NaN|    10|\n",
      "| 7788| SCOTT|  ANALYST|7566.0|1982-12-09|3000|   NaN|    20|\n",
      "| 7839|  KING|PRESIDENT|   NaN|1981-11-17|5000|   NaN|    10|\n",
      "| 7844|TURNER| SALESMAN|7698.0|1981-09-08|1500|   0.0|    30|\n",
      "| 7876| ADAMS|    CLERK|7788.0|1983-01-12|1100|   NaN|    20|\n",
      "| 7900| JAMES|    CLERK|7698.0|1981-12-03| 950|   NaN|    30|\n",
      "| 7902|  FORD|  ANALYST|7566.0|1981-12-03|3000|   NaN|    20|\n",
      "| 7934|MILLER|    CLERK|7782.0|1982-01-23|1300|   NaN|    10|\n",
      "+-----+------+---------+------+----------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Query using SparkSQL Core\n",
    "sql(\"select * from empTab\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To read json file using Spark from HDFS \n",
    "df = spark.read.json(\"data/emp.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_empdf_collection(df):\n",
    "    l = []\n",
    "    for row in df.collect():\n",
    "        for i in range(0,len(row.empno)):\n",
    "            l.append(\n",
    "                (row.empno[i],\n",
    "                 row.ename[i],\n",
    "                 row.job[i],\n",
    "                 row.mgr[i],\n",
    "                 row.hiredate[i],\n",
    "                 row.sal[i],\n",
    "                 row.comm[i],\n",
    "                 row.deptno[i])\n",
    "            )\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----------+------+------+------+\n",
      "|empno| ename|      job|   mgr|  hiredate|   sal|  comm|deptno|\n",
      "+-----+------+---------+------+----------+------+------+------+\n",
      "| 7369| SMITH|    CLERK|7902.0|1980-12-17| 800.0|  null|    20|\n",
      "| 7499| ALLEN| SALESMAN|7698.0|1981-02-20|1600.0| 300.0|    30|\n",
      "| 7876| ADAMS|    CLERK|7788.0|1983-01-12|1100.0|  null|    20|\n",
      "| 7900| JAMES|    CLERK|7698.0|1981-12-03| 950.0|  null|    30|\n",
      "| 7902|  FORD|  ANALYST|7566.0|1981-12-03|3000.0|  null|    20|\n",
      "| 7934|MILLER|    CLERK|7782.0|1982-01-23|1300.0|  null|    10|\n",
      "| 7521|  WARD| SALESMAN|7698.0|1981-02-22|1250.0| 500.0|    30|\n",
      "| 7566| JONES|  MANAGER|7839.0|1981-04-02|2975.0|  null|    20|\n",
      "| 7654|MARTIN| SALESMAN|7698.0|1981-09-28|1250.0|1400.0|    30|\n",
      "| 7698| BLAKE|  MANAGER|7839.0|1981-05-01|2850.0|  null|    30|\n",
      "| 7782| CLARK|  MANAGER|7839.0|1981-06-09|2450.0|  null|    10|\n",
      "| 7788| SCOTT|  ANALYST|7566.0|1982-12-09|3000.0|  null|    20|\n",
      "| 7839|  KING|PRESIDENT|  null|1981-11-17|5000.0|  null|    10|\n",
      "| 7844|TURNER| SALESMAN|7698.0|1981-09-08|1500.0|   0.0|    30|\n",
      "+-----+------+---------+------+----------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_l = convert_empdf_collection(df)\n",
    "emp_df = spark.createDataFrame(emp_l,(\"empno\",\"ename\",\"job\",\"mgr\",\"hiredate\",\"sal\",\"comm\",\"deptno\"))\n",
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----------+------+----+------+\n",
      "|empno| ename|      job|   mgr|  hiredate|   sal|comm|deptno|\n",
      "+-----+------+---------+------+----------+------+----+------+\n",
      "| 7934|MILLER|    CLERK|7782.0|1982-01-23|1300.0|null|    10|\n",
      "| 7782| CLARK|  MANAGER|7839.0|1981-06-09|2450.0|null|    10|\n",
      "| 7839|  KING|PRESIDENT|  null|1981-11-17|5000.0|null|    10|\n",
      "+-----+------+---------+------+----------+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df[emp_df.deptno == 10].show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
