{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spark Session using local(Standalone)\n",
    "spark = SparkSession.builder \\\n",
    "     .master(\"local[*]\") \\\n",
    "     .appName(\"Pyspark SQL\") \\\n",
    "     .config(\"conf.ui.port\", \"50032\") \\\n",
    "     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File completed: 0.7937862873077393\n"
     ]
    }
   ],
   "source": [
    "#Word count script\n",
    "#Read file and use less apis and it converts it into collection\n",
    "#Then convert collection to RDD and Use coalesce to make it as one partition and save as file\n",
    "#If you don't use it creates multiple files and also takes more time\n",
    "#By using coalesce you can reduce processing time almost 40-45%\n",
    "t = time.time()\n",
    "f = sc.textFile(\"data/file.txt\")\n",
    "result = f.flatMap(lambda line: line.split(\" \")).countByValue()\n",
    "sc.parallelize(list(result.items())).coalesce(1).saveAsTextFile(\"output/word_cnt\")\n",
    "print(\"File completed:\",time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File completed: 0.9056434631347656\n"
     ]
    }
   ],
   "source": [
    "#Other way to implement word count\n",
    "#coalesce saves almost 50% of time\n",
    "t = time.time()\n",
    "f = sc.textFile(\"data/file.txt\")\n",
    "result = f.flatMap(lambda line: line.split(\" \")).map(lambda words:(words,1)).reduceByKey(lambda a,b:a+b)\n",
    "result.coalesce(1).saveAsTextFile(\"output/word_cnt2\")\n",
    "print(\"File completed:\",time.time()-t)                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"data/emp.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_empdf_collection(df):\n",
    "    l = []\n",
    "    for row in df.collect():\n",
    "        for i in range(0,len(row.empno)):\n",
    "            l.append(\n",
    "                (row.empno[i],\n",
    "                 row.ename[i],\n",
    "                 row.job[i],\n",
    "                 row.mgr[i],\n",
    "                 row.hiredate[i],\n",
    "                 row.sal[i],\n",
    "                 row.comm[i],\n",
    "                 row.deptno[i])\n",
    "            )\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+------+----------+------+------+------+\n",
      "|empno| ename|      job|   mgr|  hiredate|   sal|  comm|deptno|\n",
      "+-----+------+---------+------+----------+------+------+------+\n",
      "| 7369| SMITH|    CLERK|7902.0|1980-12-17| 800.0|  null|    20|\n",
      "| 7499| ALLEN| SALESMAN|7698.0|1981-02-20|1600.0| 300.0|    30|\n",
      "| 7876| ADAMS|    CLERK|7788.0|1983-01-12|1100.0|  null|    20|\n",
      "| 7900| JAMES|    CLERK|7698.0|1981-12-03| 950.0|  null|    30|\n",
      "| 7902|  FORD|  ANALYST|7566.0|1981-12-03|3000.0|  null|    20|\n",
      "| 7934|MILLER|    CLERK|7782.0|1982-01-23|1300.0|  null|    10|\n",
      "| 7521|  WARD| SALESMAN|7698.0|1981-02-22|1250.0| 500.0|    30|\n",
      "| 7566| JONES|  MANAGER|7839.0|1981-04-02|2975.0|  null|    20|\n",
      "| 7654|MARTIN| SALESMAN|7698.0|1981-09-28|1250.0|1400.0|    30|\n",
      "| 7698| BLAKE|  MANAGER|7839.0|1981-05-01|2850.0|  null|    30|\n",
      "| 7782| CLARK|  MANAGER|7839.0|1981-06-09|2450.0|  null|    10|\n",
      "| 7788| SCOTT|  ANALYST|7566.0|1982-12-09|3000.0|  null|    20|\n",
      "| 7839|  KING|PRESIDENT|  null|1981-11-17|5000.0|  null|    10|\n",
      "| 7844|TURNER| SALESMAN|7698.0|1981-09-08|1500.0|   0.0|    30|\n",
      "+-----+------+---------+------+----------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_t = convert_empdf_collection(df)\n",
    "emp = spark.createDataFrame(emp_t,(\"empno\",\"ename\",\"job\",\"mgr\",\"hiredate\",\"sal\",\"comm\",\"deptno\"))\n",
    "emp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+------+----------+------+----+------+\n",
      "|empno|ename|    job|   mgr|  hiredate|   sal|comm|deptno|\n",
      "+-----+-----+-------+------+----------+------+----+------+\n",
      "| 7369|SMITH|  CLERK|7902.0|1980-12-17| 800.0|null|    20|\n",
      "| 7876|ADAMS|  CLERK|7788.0|1983-01-12|1100.0|null|    20|\n",
      "| 7902| FORD|ANALYST|7566.0|1981-12-03|3000.0|null|    20|\n",
      "| 7566|JONES|MANAGER|7839.0|1981-04-02|2975.0|null|    20|\n",
      "| 7788|SCOTT|ANALYST|7566.0|1982-12-09|3000.0|null|    20|\n",
      "+-----+-----+-------+------+----------+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp[emp.deptno == 20].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[empno: bigint, ename: string, job: string, mgr: double, hiredate: string, sal: double, comm: double, deptno: bigint]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emp_p = spark.read.parquet(\"data/emp.parquet\")\n",
    "emp_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------+------+----------+------+------+------+\n",
      "|empno| ename|     job|   mgr|  hiredate|   sal|  comm|deptno|\n",
      "+-----+------+--------+------+----------+------+------+------+\n",
      "| 7499| ALLEN|SALESMAN|7698.0|1981-02-20|1600.0| 300.0|    30|\n",
      "| 7521|  WARD|SALESMAN|7698.0|1981-02-22|1250.0| 500.0|    30|\n",
      "| 7654|MARTIN|SALESMAN|7698.0|1981-09-28|1250.0|1400.0|    30|\n",
      "| 7698| BLAKE| MANAGER|7839.0|1981-05-01|2850.0|  null|    30|\n",
      "| 7844|TURNER|SALESMAN|7698.0|1981-09-08|1500.0|   0.0|    30|\n",
      "| 7900| JAMES|   CLERK|7698.0|1981-12-03| 950.0|  null|    30|\n",
      "+-----+------+--------+------+----------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_p[emp_p.deptno == 30].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to stop spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
