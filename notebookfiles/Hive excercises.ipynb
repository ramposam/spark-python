{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating external table\n",
    "#externbal table must be a directory not file\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS dept_big(\n",
    "  deptno int, \n",
    "  dname string, \n",
    "  loc string)\n",
    "COMMENT 'external table with big amount of file'\n",
    "ROW FORMAT DELIMITED \n",
    "  FIELDS TERMINATED BY ',' \n",
    "  LINES TERMINATED BY '\\n' \n",
    "LOCATION\n",
    "  'hdfs://nn01.itversity.com:8020/user/rposam2020/output/dept_big';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating external table\n",
    "#externbal table must be a directory not file\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS emp_big(\n",
    "  empno int, \n",
    "  ename string, \n",
    "  job string, \n",
    "  mgr int, \n",
    "  hiredate string, \n",
    "  sal double, \n",
    "  comm double, \n",
    "  deptno int)\n",
    "COMMENT 'External table with big amount of file'\n",
    "ROW FORMAT DELIMITED \n",
    "  FIELDS TERMINATED BY ',' \n",
    "  LINES TERMINATED BY '\\n'\n",
    "LOCATION\n",
    "  'hdfs://nn01.itversity.com:8020/user/rposam2020/output/emp_big';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hive Data Flow\n",
    "#when query executed -> \n",
    "    goes to Hive Driver -> \n",
    "        Hive Driver sent query to Compiler\n",
    "             -> Metastore -> \n",
    "                 if valid -> \n",
    "                     results back to Driver\n",
    "    Driver set execution engine ->\n",
    "        Job Tracker ->\n",
    "             Map Tasks -> NameNode to get files,blocks,locations -> task done on Datanode\n",
    "             Reduce Tasks -> NameNode to get files,blocks,locations -> task done on Datanode\n",
    "        results back to job tracker\n",
    "    results back to execution enginer\n",
    "    results back to driver\n",
    "show results on UI\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Partition and Bucketing concept\n",
    "#if the source file is big then we split into 4 small files using bucketing\n",
    "CREATE  TABLE IF NOT EXISTS emp_bucket_part(\n",
    "  empno int, \n",
    "  ename string, \n",
    "  job string, \n",
    "  mgr int, \n",
    "  hiredate string, \n",
    "  sal double, \n",
    "  comm double)\n",
    " COMMENT 'table with big amount of file BUCKET AND PARTITION'\n",
    "PARTITIONED BY (deptno int)\n",
    "CLUSTERED BY (JOB) INTO 4 BUCKETS\n",
    "\n",
    "ROW FORMAT DELIMITED \n",
    "  FIELDS TERMINATED BY ',' \n",
    "  LINES TERMINATED BY '\\n'\n",
    "STORED AS ORC;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert into emp_bucket_part partition(deptno) select * from emp_big;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only bucketing\n",
    "CREATE  TABLE IF NOT EXISTS emp_part(\n",
    "  empno int, \n",
    "  ename string, \n",
    "  mgr int, \n",
    "  hiredate string, \n",
    "  sal double, \n",
    "  comm double)\n",
    " COMMENT 'table with big amount of file BUCKET AND PARTITION'\n",
    "PARTITIONED BY (deptno int,job string )\n",
    "ROW FORMAT DELIMITED \n",
    "  FIELDS TERMINATED BY ',' \n",
    "  LINES TERMINATED BY '\\n'\n",
    "STORED AS textfile;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if dynamic partition is enable then only this query works other wise static partition inserts has to be done\n",
    "insert into emp_part partition(deptno,job) \n",
    "select empno,ename,mgr,hiredate,sal,comm,deptno,job from emp_big;\n",
    "\n",
    "#static partition inserting\n",
    "insert into emp_part partition(deptno='30',job='MANAGER') \n",
    "select empno,ename,mgr,hiredate,sal,comm,deptno,job from emp_big\n",
    "WHERE DEPTNO = 30 AND JOB = 'MANAGER';\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you query table using limit it mostly returns rows related to one partition\n",
    "#so, to get rows randomly from all partitions\n",
    "select * from emp_part \n",
    "order by rand() limit 25;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only static partition will be allowed for load\n",
    "from hdfs\n",
    "load data inpath \"/user/rposam2020/output/emp_big\" overwrite into table emp_part;\n",
    "\n",
    "from local\n",
    "load data local inpath \"/user/rposam2020/data/emp.csv\" overwrite into table emp_csv;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#we can use spark2-sql instead of hive\n",
    "#spark2-sql support all queries of hive and it doesn't process data using map reduce \n",
    "#to enable user level privileges we use beeline\n",
    "!connect jdbc:hive2://rm01.itversity.com:10000\n",
    "if you don't have access to the other users tables it simply throws an error permission denied\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - SQL",
   "language": "sql",
   "name": "apache_toree_sql"
  },
  "language_info": {
   "codemirror_mode": "text/x-sql",
   "file_extension": ".sql",
   "mimetype": "text/x-sql",
   "name": "sql",
   "pygments_lexer": "sql",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
